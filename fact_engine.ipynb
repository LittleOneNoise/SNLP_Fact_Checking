{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LittleOneNoise/SNLP_Fact_Checking/blob/main/fact_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4whhbj5G84Ux",
        "outputId": "d6c9d1c9-dcbc-44a5-f197-e0e490ebd88c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.7/dist-packages (1.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2021.10.8)\n"
          ]
        }
      ],
      "source": [
        "#mount the drive and install wikipedia library\n",
        "!pip install wikipedia\n",
        "from google.colab import drive\n",
        "\n",
        "# generic libraries\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# task specific libraries\n",
        "import unicodedata\n",
        "import wikipedia as wiki\n",
        "import urllib.request\n",
        "from urllib.request import urlopen\n",
        "from urllib.error import HTTPError\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of GatherData is used to collect the raw information that was provided and extract relevant information for the fact checking task!***"
      ],
      "metadata": {
        "id": "tKo3KxX3JxEQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kW4yGka02Fob"
      },
      "outputs": [],
      "source": [
        "class GatherData:\n",
        "    def __init__(self, file_name, save_path, dataset_type='train', categories=dict()):\n",
        "        self.file_name = file_name\n",
        "        self.stop_words = ['s', 'is', 'the', 'that', 'a', 'in', \"of\"]\n",
        "        self.dataset_type = dataset_type\n",
        "        self.categories = categories\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.save_path = save_path\n",
        "\n",
        "    def collect_data(self):\n",
        "        \"\"\"\n",
        "        Method yields each line from the given file\n",
        "\n",
        "        :yield: each line from the given file\n",
        "        \"\"\"\n",
        "        with open(self.file_name) as file_test:\n",
        "            tsv_file = csv.reader(file_test, delimiter=\"\\t\")\n",
        "            for line in tsv_file:\n",
        "                yield line\n",
        "\n",
        "    def extract_information(self):\n",
        "        \"\"\"\n",
        "        Method extract needed information from raw data\n",
        "        :return: data_dict: dictionary that includes information such as raw_sentence, clean_sentence,\n",
        "        tokenized_sentence, label, category and organizations that sentences might involve\n",
        "        \"\"\"\n",
        "        data_dict = dict()\n",
        "        length_dataset = 0\n",
        "\n",
        "        for each_data in self.collect_data():\n",
        "            length_dataset += 1\n",
        "\n",
        "        processing_bar = tqdm(iterable=self.collect_data(), desc='Data is collected and processed',\n",
        "                              total=length_dataset, leave=False)\n",
        "\n",
        "        all_sentences = list()\n",
        "        for each_data in processing_bar:\n",
        "            if self.dataset_type == 'train':\n",
        "                each_id, each_sentence, each_label = each_data\n",
        "                categories = self.extract_categories()\n",
        "            else:\n",
        "                categories = self.categories\n",
        "                each_id, each_sentence = each_data\n",
        "\n",
        "            clean_sentence = self.clean_sentences(each_sentence)\n",
        "            splitted_sentence = self.tokenize_sentence(each_sentence, stopwords=False)\n",
        "            bigram_sentence, raw_bigrams = self.extract_bigrams(splitted_sentence)\n",
        "            category = self.generate_categories(categories['bigram_categories'], bigram_sentence)\n",
        "\n",
        "            if category == 'None_cat':\n",
        "                category = self.generate_categories(categories['unigram_categories'], splitted_sentence)\n",
        "\n",
        "            organizations = self.extract_organizations(each_sentence)\n",
        "\n",
        "            data_dict[each_id] = {\n",
        "                'raw_sentence': each_sentence,\n",
        "                'clean_sentence': clean_sentence,\n",
        "                'tokenized_sentence': splitted_sentence,\n",
        "                'category': category,\n",
        "                'label': each_label if self.dataset_type == 'train' else 'None',\n",
        "                'organizations_spacy': organizations\n",
        "            }\n",
        "            processing_bar.set_description(f'Data is collected and processed: {each_id}')\n",
        "\n",
        "        return data_dict\n",
        "\n",
        "    def generate_categories(self, categories, sentence):\n",
        "        \"\"\"\n",
        "        Generate category with respect to given sentence and list of categories that were extracted from dataset\n",
        "        :param categories: list of categories that were extracted from dataset (can be unigrams or bigrams)\n",
        "        :param sentence: tokenized sentence with unigrams or bigrams according to the category type\n",
        "        :return: category of the sentence if it exists, otherwise 'None_cat' stands for unsuccessful category matching\n",
        "        \"\"\"\n",
        "        cat = list()\n",
        "        count = 0\n",
        "\n",
        "        for each in sentence:\n",
        "            if each in categories:\n",
        "                count += 1\n",
        "                cat.append(each)\n",
        "                if count == 1:\n",
        "                    break\n",
        "        if count == 1:\n",
        "            return cat[0]\n",
        "        else:\n",
        "            return 'None_cat'\n",
        "\n",
        "    def extract_categories(self):\n",
        "        \"\"\"\n",
        "        Method is used to extract categories from the given tokenized sentences. It uses counter and categories were\n",
        "        extracted according to the most common words in the whole dataset.\n",
        "        Note: Categories are extracted according to the train dataset!\n",
        "        :return: dictionary of categories which has the following structure:\n",
        "        {'unigram_categories': list of unigram categories, 'bigram_categories': list of bigram_categories}\n",
        "        \"\"\"\n",
        "        bigrams = list()\n",
        "        unigrams = list()\n",
        "        length_dataset = 0\n",
        "\n",
        "        for each_data in self.collect_data():\n",
        "            bigrams.extend(self.extract_bigrams(self.tokenize_sentence(each_data[1]))[0])\n",
        "            unigrams.extend(self.tokenize_sentence(each_data[1]))\n",
        "\n",
        "        categories = dict()\n",
        "        bi_counter = Counter(bigrams)\n",
        "        bigram_categories = [each_cat for each_cat, freq in bi_counter.most_common() if\n",
        "                             freq >= 75 and each_cat != 'award nobel' and each_cat != 'prize literature' and each_cat != 'nobel prize']\n",
        "\n",
        "        uni_counter = Counter(unigrams)\n",
        "        unigram_categories = [each_cat for each_cat, freq in uni_counter.most_common() if\n",
        "                              freq >= 60 and each_cat not in ['the', 'new', 'literature', 'nobel', 'prize']]\n",
        "        unigram_categories.append('squad')\n",
        "\n",
        "        tokenized_bigrams = list()\n",
        "\n",
        "        for each in bigram_categories:\n",
        "            tokenized_bigrams.extend(self.tokenize_sentence(each))\n",
        "\n",
        "        tokbi_counter = Counter(tokenized_bigrams)\n",
        "        all_tokens_cat = [each_element for each_element, each_freq in tokbi_counter.most_common()]\n",
        "\n",
        "        categories['bigram_categories'] = bigram_categories\n",
        "        uni_cats = list()\n",
        "\n",
        "        for each_uni in unigram_categories:\n",
        "            if (each_uni not in all_tokens_cat):\n",
        "                uni_cats.append(each_uni)\n",
        "\n",
        "        categories['unigram_categories'] = uni_cats\n",
        "\n",
        "        return categories\n",
        "\n",
        "    def tokenize_sentence(self, sentence, pattern='\\W', stopwords=True):\n",
        "        \"\"\"\n",
        "        Tokenize given sentence according to the given pattern and boolean flag\n",
        "        :param sentence: given raw sentence was extracted from the dataset\n",
        "        :param pattern: given pattern is used for regex matching in the given sentence\n",
        "        :param stopwords: boolean flag determines whether stopwords should be discarded or not\n",
        "        :return: list of tokens in the sentence that includes also stopwords if flag is False\n",
        "        \"\"\"\n",
        "        if stopwords:\n",
        "            return [word.lower() for word in re.split(pattern, sentence) if word and word not in self.stop_words]\n",
        "        else:\n",
        "            return [word.lower() for word in re.split(pattern, sentence) if word]\n",
        "\n",
        "    def extract_bigrams(self, sentence):\n",
        "        \"\"\"\n",
        "        Method extracts bigrams in order to use them for category matching in future\n",
        "        :param sentence: tokenized sentence which was generated from the given raw sentence\n",
        "        :return: combined_bigram_sentence: list of string bigrams, e.g., [nobel prize]\n",
        "                 raw_bigram_sentence: list of two tokens make the bigram, e.g., [nobel, prize]\n",
        "        \"\"\"\n",
        "        combined_bigram_sentence = list()\n",
        "        raw_bigram_sentence = list()\n",
        "        for current_idx in range(len(sentence)):\n",
        "            if current_idx != len(sentence) - 1:\n",
        "                current = sentence[current_idx: current_idx + 2]\n",
        "                combined = current[0] + ' ' + current[1]\n",
        "            combined_bigram_sentence.append(combined)\n",
        "            raw_bigram_sentence.append(current)\n",
        "        return combined_bigram_sentence, raw_bigram_sentence\n",
        "\n",
        "    def clean_sentences(self, sentence):\n",
        "        \"\"\"\n",
        "        Method was generated for exemptions where sentence can include extra information after it ends\n",
        "        (e.g., Arthur C. Doyle is Scherlock Holmes' author. (author))\n",
        "        :param sentence: raw sentence was extracted from the dataset\n",
        "        :return: new_sentence: resulting sentence where the extra information was discarded\n",
        "        (e.g., Arthur C. Doyle is Scherlock Holmes' author.)\n",
        "        \"\"\"\n",
        "        limit_idx = ''\n",
        "        new_sentence = ''\n",
        "        for each_idx in range(len(sentence)):\n",
        "            new_sentence += sentence[each_idx]\n",
        "            if sentence[each_idx] == '.' and each_idx == len(sentence) - 1:\n",
        "                break\n",
        "        return new_sentence\n",
        "\n",
        "    def extract_extra_categories(self, dict_data):\n",
        "        \"\"\"\n",
        "        Method is used to gather other categories that could not be gathered because of the threshold conditioning\n",
        "        :param dict_data: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        :return: extra_categories: list of categories that were mentioned above\n",
        "        \"\"\"\n",
        "        uncat_data = dict()\n",
        "        organizations = list()\n",
        "        collection = list()\n",
        "        for each_id, each_data in dict_data.items():\n",
        "            if each_data['category'] == 'None_cat':\n",
        "                uncat_data[each_id] = each_data\n",
        "                organizations.extend(each_data['organizations_spacy'])\n",
        "                collection.extend(each_data['tokenized_sentence'])\n",
        "            uncat_collection = Counter(collection)\n",
        "            organizations = [each.lower() for each in organizations]\n",
        "            extra_categories = [each_token for each_token, each_freq in uncat_collection.most_common()\n",
        "                                if each_freq > 8]\n",
        "            extra_categories = [each_token for each_token in extra_categories\n",
        "                                if each_token not in self.stop_words and each_token not in organizations]\n",
        "        forbidden = ['nobel', 'prize']\n",
        "        extra_categories = [each_cat for each_cat in extra_categories if each_cat not in forbidden]\n",
        "        return extra_categories\n",
        "\n",
        "    def categorize_noncats(self, dict_data, main_cats, train_extras='None'):\n",
        "        \"\"\"\n",
        "        Categorize non-categorized data in the dataset by using the extra categories were generated\n",
        "        :param dict_data: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        :param main_cats: Main categories that were generated at the first phase of category matching\n",
        "        :param train_extras: Extra categories that were generated at the second phase of category matching. It is None\n",
        "                when it analyzes train data and it is generated in this function, that will be used for test dataset\n",
        "        :return:dict_data: dictionary of dataset that includes all possible valuable information from raw data (without\n",
        "                non-categorized data)\n",
        "                extra_categories: Extra categories that were generated at the second phase of category matching for \n",
        "                further uses\n",
        "        \"\"\"\n",
        "        if train_extras == 'None':\n",
        "            extra_categories = self.extract_extra_categories(dict_data)\n",
        "        else:\n",
        "            extra_categories = train_extras\n",
        "        for each_id, each_data in dict_data.items():\n",
        "            if each_data['category'] == 'None_cat':\n",
        "                sentence = each_data['tokenized_sentence']\n",
        "                each_data['category'] = self.generate_categories(extra_categories, sentence)\n",
        "                if each_data['category'] == 'None_cat':\n",
        "                    each_data['category'] = self.generate_categories(main_cats, sentence)\n",
        "\n",
        "        return dict_data, extra_categories\n",
        "\n",
        "    def extract_organizations(self, sentence):\n",
        "        \"\"\"\n",
        "        Extract organization by using Spacy library's NER property. It is used to prevent to use organization names,\n",
        "        which were used mostly in sentences, as categories\n",
        "        :param sentence: raw sentence was extracted from the raw data\n",
        "        :return: organizations: list of tokens were classified as organization names.\n",
        "        \"\"\"\n",
        "        tokenized_sentence = self.nlp(sentence)\n",
        "        organizations = [e.text for e in tokenized_sentence.ents if e.label_ == 'ORG']\n",
        "\n",
        "        for current_org in range(len(organizations)):\n",
        "            if \"'s\" in organizations[current_org]:\n",
        "                organizations[current_org] = organizations[current_org].replace(\"'s\", '')\n",
        "        return organizations\n",
        "\n",
        "    def check_ambiguities(self, data_dict):\n",
        "        \"\"\"\n",
        "        Prevents ambiguities that can happen when there are 2 categories in sentence but only one of them should be \n",
        "        taken into consideration as category\n",
        "        :param data_dict: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        :return: data_dict: dictionary of dataset that includes all possible valuable information from raw data where\n",
        "                ambiguities were resolved\n",
        "        \"\"\"\n",
        "        for each_id, each_data in data_dict.items():\n",
        "            category = each_data['category']\n",
        "            if f'({category})' in each_data['raw_sentence']:\n",
        "                if not f' {category} ' in each_data['raw_sentence'] or not f' {category}.' in each_data['raw_sentence']:\n",
        "                    each_data['category'] = 'None_cat'\n",
        "        return data_dict\n",
        "\n",
        "    def save_and_load(self):\n",
        "        \"\"\"\n",
        "        Generates dataset if the given file path does not exists and save it. However, it path exists it loads dataset\n",
        "        and returns it as a dictionary.\n",
        "        :return: dataset: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        \"\"\"\n",
        "        file_name = os.path.join(self.save_path, f'{self.dataset_type}.json')\n",
        "\n",
        "        if os.path.exists(file_name):\n",
        "            with open(file_name) as saved_file:\n",
        "                dataset = json.load(saved_file)\n",
        "        else:\n",
        "            data_dict = self.extract_information()\n",
        "            dataset = self.check_ambiguities(data_dict)\n",
        "            with open(file_name, 'w') as fname:\n",
        "                json.dump(dataset, fname)\n",
        "        return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of ProcessData is used to process the extracted information in order to generate more specific information!***"
      ],
      "metadata": {
        "id": "81KozBM6KRdi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwLw5v2kersp"
      },
      "outputs": [],
      "source": [
        "class ProcessData:\n",
        "    def __init__(self, saved_path, filetype, collector):\n",
        "        self.saved_path = saved_path\n",
        "        self.stop_words = ['s', 'is']\n",
        "        self.nlp = spacy.load('en_core_web_sm')\n",
        "        self.filetype = filetype\n",
        "        self.collector = collector\n",
        "\n",
        "    def clean_data(self, dataset, extra_categories):\n",
        "        \"\"\"\n",
        "        Categorizing non-categorized data, extra information generation were done in this method\n",
        "        :param dataset: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        :param extra_categories: extra categories were generated by using the class of GatherData\n",
        "        :return: new_dataset: dictionary of dataset that includes all possible valuable information from raw data that\n",
        "                includes all categorized data and extra information that sentences included\n",
        "        \"\"\"\n",
        "        new_dataset = dict()\n",
        "        progress_bar = tqdm(dataset.items(), desc='Data is processed for secondary info: None', total=len(dataset))\n",
        "        for each_id, each_data in progress_bar:\n",
        "            progress_bar.desc = f'Data is processed for secondary info: {each_id}'\n",
        "            each_data['extra_info'] = self.new_extra_generator(each_data['raw_sentence'], each_data['category'])\n",
        "            new_dataset[each_id] = each_data\n",
        "        new_dataset = self.fix_categories(new_dataset, extra_categories, afterall=True)\n",
        "        return new_dataset\n",
        "\n",
        "    def fix_categories(self, dataset, main_categories, afterall=False):\n",
        "        \"\"\"\n",
        "        Method is used in order to solve further category disambiguation that was observed from dataset\n",
        "        :param dataset: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        :param main_categories: all categories that were generated at the first phase of the category matching\n",
        "        :param afterall: boolean flag determines when it should be called such as at the beginning of class\n",
        "                        initialization or at the end of it\n",
        "        :return: dataset: dictionary of dataset that includes all possible valuable information from raw data\n",
        "        \"\"\"\n",
        "        for each_id, data in dataset.items():\n",
        "            for category in main_categories:\n",
        "                if category in data['raw_sentence'] and f'({category})' not in data['raw_sentence']:\n",
        "                    data['category'] = category\n",
        "            if afterall:\n",
        "                dataset[each_id] = self.resulting_dataset(data)\n",
        "        return dataset\n",
        "\n",
        "    def new_extra_generator(self, sentence, category):\n",
        "        \"\"\"\n",
        "        Method is used to solve issues of extra information extraction from the sentence when it has different structure\n",
        "        than usual (e.g., Paris is Charle De Gaulle's birt place.)\n",
        "        :param sentence: raw sentence was extracted from the dataset\n",
        "        :param category: category which is relevant to the sentence is issued\n",
        "        :return: if there is extra information match according to the utilized regex, then returns the extra information\n",
        "                else it returns 'No_extra' that stands for unsuccessful extra information generation\n",
        "                Note: if sentence structure is usual it uses extra_info_generator() method to solve it\n",
        "        \"\"\"\n",
        "        if sentence[-1] != '.':\n",
        "            sentence += '.'\n",
        "        split_sentence = sentence.split(f' {category}')\n",
        "        if '.' in split_sentence:\n",
        "            clean_sentence = re.sub(\"'s\", '', split_sentence[0])\n",
        "            new_split = clean_sentence.split('is ')\n",
        "            matches = list()\n",
        "            for each in new_split:\n",
        "                match = re.findall(\"(.*)\\s\\(([^)]+)\\)\", each, re.IGNORECASE)\n",
        "\n",
        "                if match:\n",
        "                    matches.append(match[0])\n",
        "            return matches if matches else 'No_extra'\n",
        "        else:\n",
        "            return self.extra_info_generator(sentence, category)\n",
        "\n",
        "    def extra_info_generator(self, sentence, category):\n",
        "        \"\"\"\n",
        "        Extracts extra information from the given sentence with respect to the category of sentence, if sentence has\n",
        "        usual structure (e.g., Charle De Gaulle's birth place is Paris.)\n",
        "        :param sentence: raw sentence that was extracted from the dataset\n",
        "        :param category: category of the sentence\n",
        "        :return: extra information if there is a match according to the utilized regex pattern, else\n",
        "                it returns 'No_extra' that stands for no extra matches was found\n",
        "        \"\"\"\n",
        "\n",
        "        clean_sentence = re.sub(\"'s| is\", '', sentence)\n",
        "        split_cat = clean_sentence.split(f' {category} ')\n",
        "        matches = list()\n",
        "        for each in split_cat:\n",
        "            match = re.findall(\"(.*)\\s\\(([^)]+)\\)\", each, re.IGNORECASE)\n",
        "            if match:\n",
        "                matches.append(match[0])\n",
        "\n",
        "        if matches:\n",
        "            return matches\n",
        "        else:\n",
        "            return 'No_extra'\n",
        "\n",
        "    def tokenize_sentence(self, sentence):\n",
        "        \"\"\"\n",
        "        Method tokenizes the raw sentence and discards '' elements from the tokens\n",
        "        :param sentence: raw sentence was extracted from the database\n",
        "        :return: list of tokens were included by sentence\n",
        "        \"\"\"\n",
        "        return [each_word for each_word in re.split('\\W', sentence) if each_word != '']\n",
        "\n",
        "    def clean_tokenized(self, each_data):\n",
        "        \"\"\"\n",
        "        Method discards extra information from tokenized sentence\n",
        "        :param each_data: dictionary of information is relevant to data\n",
        "        :return: list of tokens that does not include extra information\n",
        "        \"\"\"\n",
        "        if each_data['extra_info'] != 'No_extra':\n",
        "            match = each_data['extra_info']\n",
        "            for each_match in match:\n",
        "                sent = each_data['clean_sentence'].replace(f' ({each_match[1]})', '')\n",
        "                each_data['clean_sentence'] = sent\n",
        "        return self.tokenize_sentence(each_data['clean_sentence'])\n",
        "\n",
        "    def resulting_dataset(self, each_data):\n",
        "        \"\"\"\n",
        "        Generates resulting dataset includes also secondary information where secondary information is core of the\n",
        "        fact checking methods\n",
        "        :param each_data: dictionary of information is relevant to data\n",
        "        :return: each_data: dictionary of information is relevant to data from dataset with secondary information\n",
        "        \"\"\"\n",
        "        each_data['tokenized_sentence'] = self.clean_tokenized(each_data)\n",
        "        splitted, separated = self.extract_reliable_new(each_data)\n",
        "        each_data['secondary_information'] = separated\n",
        "        each_data['secondary_information_string'] = splitted\n",
        "        return each_data\n",
        "\n",
        "    def extract_reliable_new(self, data):\n",
        "        \"\"\"\n",
        "        Method is used to extract information from the raw sentence by using regex and methods belong to str class\n",
        "        :param data: dictionary of all information relevant to data\n",
        "        :return: if sentence structure does not belong 2 general structure it returns ['None', 'None'] and\n",
        "                [['None'], ['None']], where they stand for list of strings and list of list of tokens, respectively;\n",
        "                else: splitted_sentence: list of strings (information)\n",
        "                     separated: list of list of tokens (of information)\n",
        "        \"\"\"\n",
        "        clean_sentence = data['clean_sentence']\n",
        "        if clean_sentence[-1] != '.':\n",
        "            clean_sentence += '.'\n",
        "        splitted_sentence = clean_sentence.split(f\" {data['category']}\")\n",
        "        if len(splitted_sentence) == 2:\n",
        "            if splitted_sentence[1] == '.':\n",
        "                splitted_sentence = splitted_sentence[0].split(' is')\n",
        "            else:\n",
        "                splitted_sentence[-1] = splitted_sentence[-1][:-1]\n",
        "            separated = list()\n",
        "            for idx in range(len(splitted_sentence)):\n",
        "                splitted_sentence[idx] = re.sub(\" is |'s|'\", '', splitted_sentence[idx])\n",
        "                if splitted_sentence[idx][0] == ' ':\n",
        "                    splitted_sentence[idx] = splitted_sentence[idx][1:]\n",
        "                separated.append(splitted_sentence[idx].split(' '))\n",
        "            return splitted_sentence, separated\n",
        "        else:\n",
        "            return ['None', 'None'], [['None'], ['None']]\n",
        "\n",
        "    def distinguish(self, dictionary_data, number=20):\n",
        "        \"\"\"\n",
        "        Method is used for visualization purposes of data and information were extracted from raw data\n",
        "        :param dictionary_data: dictionary of all data that include all relevant information to itself\n",
        "        :param number: number of elements that is printed for visualization\n",
        "        :return: None\n",
        "        \"\"\"\n",
        "        print(f'The first {number} data of {self.filetype} dataset:')\n",
        "        count_none = 0\n",
        "        count_cat = 0\n",
        "        all_count = 0\n",
        "        for each_id in dictionary_data.keys():\n",
        "            all_count += 1\n",
        "            print(f'{each_id}: {dictionary_data[each_id]}')\n",
        "            if dictionary_data[each_id]['category'] == 'None_cat':\n",
        "                count_none += 1\n",
        "            else:\n",
        "                count_cat += 1\n",
        "\n",
        "            if all_count == number:\n",
        "                break\n",
        "        print(f'Number of categorized data: {count_cat} :::: Number of non-categorized data: {count_none} out of {number} data in {self.filetype} dataset')\n",
        "\n",
        "    def save_and_load(self, main_categories, categories='None'):\n",
        "        \"\"\"\n",
        "        Method is used to save and load all dataset after all processing steps. Since this method is used in order to\n",
        "        collect data, last steps of processing such as fixing categorization ambiguities was done in this method\n",
        "        :param main_categories: all categories were generated at the first phase of the category matching\n",
        "        :param categories: categories were generated at the second phase of the category matching\n",
        "        :return: processed_dataset: dictionary of all data with their relevant information after main processing phases\n",
        "                 extra_categories: categories were generated at the second phase of category matching if it does not \n",
        "                 exists, else it is given as an argument and returned as untouched\n",
        "        \"\"\"\n",
        "        raw_dataset = self.collector.save_and_load()\n",
        "        extra_categories = list()\n",
        "        dataset = self.fix_categories(raw_dataset, main_categories)\n",
        "        categorized_dataset, extra_categories = self.collector.categorize_noncats(dataset, main_categories, categories)\n",
        "        file_name = os.path.join(self.saved_path, f'{self.filetype}.json')\n",
        "        processed_dataset = dict()\n",
        "        if os.path.exists(file_name):\n",
        "            with open(file_name) as saved_file:\n",
        "                processed_dataset = json.load(saved_file)\n",
        "        else:\n",
        "            processed_dataset = self.clean_data(categorized_dataset, extra_categories)\n",
        "            with open(file_name, 'w') as saved_file:\n",
        "                json.dump(processed_dataset, saved_file)\n",
        "        self.distinguish(processed_dataset, 5) # the method distinguish accepts the number of the data you want to visualize. If you put len(processed_dataset) it will show all dataset\n",
        "        return processed_dataset, extra_categories\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of VisualizeData is used to express the graphics according to the main information that datasets included such as categories and labels!***"
      ],
      "metadata": {
        "id": "fY7L1SI6Kkt8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XISbM39IXlll"
      },
      "outputs": [],
      "source": [
        "class VisualizeData:\n",
        "  def __init__(self, dataset):\n",
        "    self.dataset = dataset\n",
        "   \n",
        "  def elements_by_categories(self, categories):\n",
        "    \"\"\"\n",
        "    Generates list of tuples that each tuple has configuration \n",
        "    of (category, number of elements)\n",
        "\n",
        "    Args:\n",
        "        (list, categories): List of all categories were used.\n",
        "    Returns:\n",
        "        (list, number_of_elements): List of tuples consists category name and frequency\n",
        "        (str, ''): Text information about distribution\n",
        "    \"\"\" \n",
        "    counter_dict = list()\n",
        "    for each_id, each_data in self.dataset.items():\n",
        "      counter_dict.append(str(each_data['category']))\n",
        "    number_of_elements = Counter(counter_dict)\n",
        "\n",
        "    \n",
        "    return number_of_elements, 'Distribution of categories'\n",
        "    \n",
        "  def get_exact_data(self, category):\n",
        "    \"\"\"\n",
        "    Prints relevant data according to the given category\n",
        "\n",
        "    Args:\n",
        "        (str, category): Category out of all kind of categories\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\" \n",
        "    for each_id, each_data in self.dataset.items():\n",
        "      if each_data['category'] == category:\n",
        "        print(each_data)\n",
        "        print(20 * '-' + 20 * '=' + 20 * '-')\n",
        "  \n",
        "  def elements_by_labels(self):\n",
        "    \"\"\"\n",
        "    Generates list of tuples that each tuple has configuration \n",
        "    of (label, number of elements)\n",
        "\n",
        "    Args:\n",
        "      \n",
        "    Returns:\n",
        "        (list, number_of_elements): List of tuples consists label and frequency\n",
        "        (str, ''): Text information about distribution\n",
        "    \"\"\" \n",
        "\n",
        "    labels = ['0.0', '1.0']\n",
        "    num_labels = list()\n",
        "    for each_id, each_data in self.dataset.items():\n",
        "      num_labels.append(each_data['label'])\n",
        "    number_of_elements = Counter(num_labels)\n",
        "   \n",
        "    return number_of_elements, 'Distribution of labels'\n",
        "  \n",
        "  def plot_bars(self, data, text_info, typeset):\n",
        "    \"\"\"\n",
        "    Plots bar graph that each bar shows relevant information\n",
        "\n",
        "    Args:\n",
        "        (list, data): list of tuples such that (name, frequency)\n",
        "        (str, text_info): text information for graph label\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\" \n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_axes([0,0,1,1])\n",
        "    plt.title(f'{text_info} over {typeset} dataset')\n",
        "    names = list()\n",
        "    numbers = list()\n",
        "    for each_data, each_freq in data.items():\n",
        "      names.append(each_data + f' ({str(each_freq)})')\n",
        "      numbers.append(each_freq)\n",
        "    \n",
        "    ax.bar(names,numbers)\n",
        "    ax.autoscale(tight=False)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of GetLinks is used to generate relevant wikipedia links for each main information that correspond to each data in datasets, in order to prevent PageError and DisambiguationError that may occur when web-scrapping is performed.***"
      ],
      "metadata": {
        "id": "ZD_RX9_7K8ju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7uLCrk1XWG4"
      },
      "outputs": [],
      "source": [
        "class GetLinks:\n",
        "    def __init__(self, saved_path, dataset, stop_words):\n",
        "        self.saved_path = saved_path\n",
        "        self.dataset = dataset\n",
        "        self.stop_words = stop_words\n",
        "\n",
        "    def do_scraping(self):\n",
        "        \"\"\"\n",
        "        Method is used for generating the dictionary includes all relevant information of each data, including\n",
        "        relevant html link for wikipedia page\n",
        "        :return: dict: result_of_search: is a dictionary of three dictionaries:\n",
        "                       result_of_search = {\n",
        "                            'zeros': data that any relevant link match could not be found\n",
        "                            'one': data that only one link was found for each data\n",
        "                            'all': all data that have not any link, or one link\n",
        "                       }\n",
        "        \"\"\"\n",
        "        result_of_search = dict()\n",
        "        counter = 0\n",
        "        processing_bar = tqdm(enumerate(self.dataset.items()), 'Not found error in data: 0', len(self.dataset))\n",
        "        one_res = dict()\n",
        "        main_res = dict()\n",
        "        zero_res = dict()\n",
        "\n",
        "        for idx, (each_id, each_data) in processing_bar:\n",
        "            result = dict()\n",
        "            category = each_data['category']\n",
        "            each_data['merged_secondary_information'] = self.merge_secondary(each_data)\n",
        "            if each_data['secondary_information_string'] != ['None', 'None']:\n",
        "                result = self.search_info(each_data)\n",
        "                each_data['wiki_match'] = list(result.values())\n",
        "                if 'sorry_not_found' in result.values():\n",
        "                    counter += 1\n",
        "                    print(f\"merged info: {each_data['merged_secondary_information']}===> results: {result}\")\n",
        "                    zero_res[each_id] = {'data': each_data, 'results': result}\n",
        "                else:\n",
        "                    one_res[each_id] = {'data': each_data, 'results': result}\n",
        "            else:\n",
        "                each_data['wiki_match'] = ['sorry not found', 'sorry not found']\n",
        "                for idx, each in enumerate(each_data['secondary_information_string']):\n",
        "                    result[each] = each_data['wiki_match'][idx]\n",
        "\n",
        "            main_res[each_id] = {'data': each_data, 'results': result}\n",
        "            processing_bar.desc = f'Not found error in data: {counter}'\n",
        "            result_of_search = {'zero': zero_res, 'one': one_res, 'all': main_res}\n",
        "\n",
        "        return result_of_search\n",
        "\n",
        "    def check_structure(self, result):\n",
        "        \"\"\"\n",
        "        Since each sentence includes 2 information, we need to find links for each information. Method is used as\n",
        "        preventative function that prevents possible errors because of str or list confusion at the end of processing\n",
        "        the link.\n",
        "        :param result: str or list: resulting wiki link after all processing\n",
        "        :return: str: if one of the links is list of one element, method returns str, otherwise returns result itself\n",
        "        \"\"\"\n",
        "        if type(result) == str:\n",
        "            return result\n",
        "        else:\n",
        "            return result[0]\n",
        "\n",
        "    def merge_secondary(self, data):\n",
        "        \"\"\"\n",
        "        Method is used to merge string information and its extra information (if exists), because it may appear in wiki\n",
        "        link with extra information of the given information\n",
        "        :param data: each data in the main dataset that includes all relevant and extracted information from the given sentence\n",
        "        :return: if there is not extra info, it returns list of all string information,\n",
        "                else list of all information (after merging them with relevant extra information is included by sentence)\n",
        "        \"\"\"\n",
        "        if data['extra_info'] == 'No_extra':\n",
        "            return data['secondary_information_string']\n",
        "        else:\n",
        "            resulting = dict()\n",
        "            for each_secondary in data['secondary_information_string']:\n",
        "                for each_extra_info in data['extra_info']:\n",
        "                    if each_secondary in each_extra_info:\n",
        "                        resulting[each_secondary] = each_extra_info[0] + f' ({each_extra_info[1]})'\n",
        "                        break\n",
        "                    else:\n",
        "                        resulting[each_secondary] = each_secondary\n",
        "            return list(resulting.values())\n",
        "\n",
        "    def search_info(self, data):\n",
        "        \"\"\"\n",
        "        This method is the key function to find relevant wiki links with respect to provided data\n",
        "        If the raw information is in the results list from wikipedia that is considered as true information,\n",
        "        else information will be sent to self.handle_not_matches(...) for further procedures\n",
        "        :param data: dictionary of all information was extracted from the given sentence\n",
        "        :return: result_dict: the same dictionary as given argument with addition of wiki links of information\n",
        "        \"\"\"\n",
        "        merged_info = data['merged_secondary_information']\n",
        "        secondary_info_sep = data['secondary_information']\n",
        "        secondary_info_string = data['secondary_information_string']\n",
        "        extra_info = data['extra_info']\n",
        "        result_dict = dict()\n",
        "\n",
        "        for each_info, each_sep, each_sec in zip(merged_info, secondary_info_sep, secondary_info_string):\n",
        "            results = wiki.search(each_info)\n",
        "            if each_info in results:\n",
        "                result_dict[each_info] = re.sub(' ', '_', self.check_structure(each_info))\n",
        "            else:\n",
        "                check_extra = self.handle_not_matches(each_info, each_sec, extra_info, results, data)\n",
        "                result_dict[each_info] = re.sub(' ', '_', self.check_structure(\n",
        "                    check_extra)) if check_extra != None else 'sorry_not_found'\n",
        "        return result_dict\n",
        "\n",
        "    def handle_not_matches(self, each_info, str_info, extra_info, results, data):\n",
        "        \"\"\"\n",
        "        Further procedures will be handled by this method if raw information could not be found in the list of\n",
        "        wiki results\n",
        "        :param each_info: list of merged information (either with extra info or itself)\n",
        "        :param str_info: list of information where each information is string (e.g., ['Antoine Lavoiser', 'John Dalton'])\n",
        "        :param extra_info: list of extra information which is relevant to the given information\n",
        "        :param results: list of results which was generated by using the main information extracted from the sentence\n",
        "        :param data: dictionary of all relevant information which were extracted from the related raw sentence\n",
        "        :return: list of link information if it was found, else 'sorry_not_found'\n",
        "        \"\"\"\n",
        "        if extra_info != 'No_extra':\n",
        "            return self.handle_extra(extra_info, str_info, results)\n",
        "        else:\n",
        "            result = self.handle_others(each_info, results, data)\n",
        "            if not result:\n",
        "                return 'sorry not found'\n",
        "            else:\n",
        "                return result\n",
        "\n",
        "    def handle_extra(self, extra_info, str_info, results):\n",
        "        \"\"\"\n",
        "        Method is used to extract the link information if information has extra information\n",
        "        :param extra_info: list of extra information which is relevant to the given information\n",
        "        :param str_info: list of information where each information is string (e.g., ['Antoine Lavoiser', 'John Dalton'])\n",
        "        :param results: list of results which was generated by using the main information extracted from the sentence\n",
        "        :return: list of resulting link information if it is found without using the extra information,\n",
        "                 else if link information as string if there is a match in results list, otherwise 'sorry_not_found'\n",
        "        \"\"\"\n",
        "        for each in extra_info:\n",
        "            if str_info in each:\n",
        "                without_extra_match = [res for res in results if res == str_info]\n",
        "                if without_extra_match:\n",
        "                    return without_extra_match\n",
        "                else:\n",
        "                    matched_lists = [(self.matcher(each_res), each_res) for each_res in results if\n",
        "                                     self.matcher(each_res) if each[0] in self.matcher(each_res)]\n",
        "                    if len(matched_lists) == 0:\n",
        "                        return 'sorry not found'\n",
        "                    elif len(matched_lists) == 1:\n",
        "                        return matched_lists[0][1]\n",
        "                    else:\n",
        "                        check_exact = [each_match for each_match in matched_lists if tuple(each) == each_match[0]]\n",
        "                        if len(check_exact) == 0:\n",
        "                            return matched_lists[0][1]\n",
        "                        else:\n",
        "                            return check_exact[0][1]\n",
        "\n",
        "    def handle_others(self, merged_info, results, data):\n",
        "        \"\"\"\n",
        "        Method is used as main method to control all methods for finding relevant links with respect to the provided\n",
        "        information such as self.no_existence, self.max_counter, self.url_matching\n",
        "        :param merged_info:\n",
        "        :param sep_info: list of lists, where each link includes string element of the main information\n",
        "                         (e.g., ['Antoine Lavoiser', 'John Dalton'] => [['Antoine', 'Lavoiser'], ['John', 'Dalton']])\n",
        "        :param results: list of results which was generated by using the main information extracted from the sentence\n",
        "        :param data: dictionary of all relevant information which were extracted from the related raw sentence\n",
        "        :return: list of wiki link matches if exists, otherwise 'sorry_not_found'\n",
        "        \"\"\"\n",
        "        if merged_info not in data['raw_sentence']:\n",
        "            new_info = self.no_existence(merged_info, data)\n",
        "            new_results = wiki.search(new_info)\n",
        "\n",
        "            result = new_results[0] if new_info in new_results else self.max_counter(new_info, new_results)\n",
        "            return result\n",
        "\n",
        "        else:\n",
        "            result = self.url_matching(merged_info, results)\n",
        "            if result:\n",
        "                return result\n",
        "            else:\n",
        "                result = self.max_counter(merged_info, results)\n",
        "                if result:\n",
        "                    return result\n",
        "                else:\n",
        "                    return 'sorry not found'\n",
        "\n",
        "    def matcher(self, information):\n",
        "        \"\"\"\n",
        "        Method generates extra information by using the pattern of \"(.*)\\s\\(([^)]+)\\)\"\n",
        "        from wiki links, if it exists\n",
        "        :param information: wiki link which is found from wikipedia by using the information that was extracted from\n",
        "                            the given sentence\n",
        "        :return: main information itself, if it has also extra information\n",
        "                 else it returns None (since it does not have match)\n",
        "        \"\"\"\n",
        "        clean_information = re.sub(\"'s\", '', information)\n",
        "        match = re.findall(\"(.*)\\s\\(([^)]+)\\)\", clean_information, re.IGNORECASE)\n",
        "        if match:\n",
        "            return match[0]\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def max_counter(self, info, result_data):\n",
        "        \"\"\"\n",
        "        Method is used to find relevant link by using string information. Information is split to strings that makes the\n",
        "        information which are going to be counted by looking for each string in the given possible link result in list\n",
        "        of results.\n",
        "        E.g., information 'Antoine Lavoiser' => ['Antoine', 'Lavoiser' ]\n",
        "              result_data ['Antoine Lavoiser (chemist)', 'Lavoiser'] => as a result of checking, the first element has\n",
        "              2 matches but the second one will have only one. Thus, the first element will be chosen as a relevant link\n",
        "        :param info: main information which is string\n",
        "        :param result_data: list of results which were extracted from the wikipedia according to the provided\n",
        "               information\n",
        "        :return: the first element of list of elements have the same number of matches if there is non-zero counter,\n",
        "                 else it returns string of '' which stands for no results\n",
        "        \"\"\"\n",
        "        splitted_info = info.split(' ')\n",
        "        match_dict = dict()\n",
        "\n",
        "        for each_res in result_data:\n",
        "            counter = 0\n",
        "            if '(disambiguation)' in each_res:\n",
        "                continue\n",
        "            else:\n",
        "                for each in splitted_info:\n",
        "                    if each in each_res:\n",
        "                        counter += 1\n",
        "            match_dict[each_res] = counter\n",
        "        if len(match_dict) == 0:\n",
        "            return ''\n",
        "        else:\n",
        "            max_val = max(match_dict.values())\n",
        "            results_list = [res for res, freq in match_dict.items() if freq == max_val]\n",
        "            return results_list[0]\n",
        "\n",
        "    def url_matching(self, merged_info, results):\n",
        "        \"\"\"\n",
        "        Method is used to make cross checking of two different methods of generation of wikipedia links in order to\n",
        "        eliminate compatibility issues such as wiki.DisambiguationError and wiki.PageError. Notice that this function\n",
        "        is used to generate wiki links which were not be able to be generated by using previous methods.\n",
        "        :param merged_info: list of merged information (either with extra info or itself)\n",
        "        :param results: list of results which was generated by using the main information extracted from the sentence\n",
        "        :return: str: link information if it was found, otherwise None\n",
        "        \"\"\"\n",
        "        try:\n",
        "            url_info = wiki.page(merged_info).url\n",
        "        except wiki.PageError as e:\n",
        "            url_info = 'None'\n",
        "        except wiki.DisambiguationError as e:\n",
        "            url_info = 'None'\n",
        "        for each in results:\n",
        "            result_url = ''\n",
        "            try:\n",
        "                result_url = wiki.page(each).url\n",
        "            except wiki.PageError as e:\n",
        "                continue\n",
        "            except wiki.DisambiguationError as e:\n",
        "                continue\n",
        "            if url_info == result_url:\n",
        "                return each\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "   \n",
        "\n",
        "    def no_existence(self, merged_info, data):\n",
        "        \"\"\"\n",
        "        Method is used for extracting merged information if the generated merged info does not exist in list of result\n",
        "        because of disambiguation of some characters was not included by generation of extra information\n",
        "        :param merged_info: list of merged information (either with extra info or itself)\n",
        "        :param data: dictionary of all relevant information which were extracted from the related raw sentence\n",
        "        :return: new merged information which is free of possible disambiguation of characters\n",
        "        \"\"\"\n",
        "        new_merge = merged_info.split(' ')\n",
        "        new_sent = data['raw_sentence'].split(' ')\n",
        "        current_idx = 0\n",
        "        req_idx = 0\n",
        "        for idx in range(len(new_merge)):\n",
        "            if new_merge[idx] in data['raw_sentence']:\n",
        "                for each in new_sent:\n",
        "                    if new_merge[idx] in each:\n",
        "                        req_idx = new_sent.index(each)\n",
        "                        current_idx = idx\n",
        "\n",
        "        req_data = new_sent[req_idx:req_idx + len(new_merge)] if current_idx == 0 else new_sent[req_idx - current_idx:req_idx] + new_sent[req_idx: req_idx + len(new_merge) - current_idx]\n",
        "        if \"'s\" in req_data[-1]:\n",
        "            req_data[-1] = re.sub(\"'s\", '', req_data[-1])\n",
        "        if \".\" in req_data[-1]:\n",
        "            req_data[-1] = req_data[-1][:-1]\n",
        "\n",
        "        return ' '.join(req_data)\n",
        "\n",
        "    def save_and_load(self, filename):\n",
        "        \"\"\"\n",
        "        Method is used to save and load the main dictionary includes also link information. Since link generation\n",
        "        consumes too much time, it is handy to implement this method.\n",
        "        :param filename: string path of the file name\n",
        "        :return: resulting dictionary which includes all data, where each data is dictionary of all relevant information\n",
        "                 which were extracted from the related raw sentence\n",
        "        \"\"\"\n",
        "        result_scrape = dict()\n",
        "        filepath = os.path.join(self.saved_path, filename)\n",
        "        if os.path.exists(filepath):\n",
        "            with open(filepath) as savedfile:\n",
        "                result_scrape = json.load(savedfile)\n",
        "        else:\n",
        "            result_scrape = self.do_scraping()\n",
        "            with open(filepath, 'w') as savedfile:\n",
        "                json.dump(result_scrape, savedfile)\n",
        "\n",
        "        return result_scrape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of LocalDatabase is used to label the test data by checking the possible matches through local database where sentences and labels were provided (i.e., train dataset)!***"
      ],
      "metadata": {
        "id": "v1palhjgLa5W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaMX2moCFR0v"
      },
      "outputs": [],
      "source": [
        "class LocalDatabase:\n",
        "    def __init__(self, saved_path):\n",
        "        self.saved_path = saved_path\n",
        "        self.counter = 0\n",
        "        self.unbelievable = 0\n",
        "        self.pairs_count = 0\n",
        "        self.pairs = [('birth place', 'nascence place'), ('death place', 'last place'), ('team', 'squad')]\n",
        "\n",
        "    def generate_database(self, dataset):\n",
        "        \"\"\"\n",
        "        Method is used to generate database which will be used before doing web-scrapping in order to check the fact. If\n",
        "        the saved file of database exists it will be loaded directly, otherwise it will be generated and saved to the \n",
        "        provided path. \n",
        "        :param dataset: dictionary which includes all data, where each data is dictionary of all relevant data\n",
        "                        which were extracted from the related raw sentence\n",
        "        :return: database is the dictionary where main data of each data were prioritized according to the\n",
        "                 structure of the sentence\n",
        "        \"\"\"\n",
        "        filename = os.path.join(saved_path, 'train_database.json')\n",
        "        database = dict()\n",
        "        if os.path.exists(filename):\n",
        "            with open(filename) as savefile:\n",
        "                database = json.load(savefile)\n",
        "        else:\n",
        "            database_process = tqdm(dataset.items(), 'Database is generated!', len(dataset))\n",
        "            for id, values in database_process:\n",
        "                database[id] = values['data']\n",
        "                if database[id]['secondary_information_string'] != ['None', 'None']:\n",
        "                    database[id]['secondary_information_string'], database[id]['secondary_information'], database[id][\n",
        "                        'merged_secondary_information'], database[id]['wiki_match'] = self.prioritize_information(\n",
        "                        values['data'])\n",
        "\n",
        "            with open(filename, 'w') as savefile:\n",
        "                json.dump(database, savefile)\n",
        "\n",
        "        return database\n",
        "\n",
        "    def prioritize_information(self, data):\n",
        "        \"\"\"\n",
        "        Method is used to prioritize the provided information according to the relevant sentence structure.\n",
        "        E.g., sentence 1: Albert Einstein's honour is Nobel Prize in Physics.;\n",
        "              sentence 2: Nobel Prize in Physics is Albert Einstein's honour. \n",
        "              More prioritized information is Albert Einstein, less prioritized information is Nobel Prize in Physics\n",
        "\n",
        "        :param data: dictionary of all relevant data which were extracted from the related raw sentence\n",
        "        :return: secondary_information_string: list of prioritized main information which are strings\n",
        "                 secondary_information: list of prioritized main information which are lists of strings\n",
        "                 merged_secondary_information: list of prioritized main information which are strings of merged\n",
        "                                               information (either with or without extra information)\n",
        "                 wiki_match: list of prioritized main information which are strings of wikipedia links\n",
        "        \"\"\"\n",
        "        sentence = data['raw_sentence']\n",
        "        category = data['category']\n",
        "\n",
        "        secondary_information_string = data['secondary_information_string']\n",
        "        secondary_information = data['secondary_information']\n",
        "        merged_secondary_information = data['merged_secondary_information']\n",
        "        wiki_match = data['wiki_match']\n",
        "\n",
        "        if sentence[-1] != '.':\n",
        "            sentence += '.'\n",
        "        elements = sentence.split(category)\n",
        "        if elements[1] == '.':\n",
        "            secondary_information_string = secondary_information_string[::-1]\n",
        "            secondary_information = secondary_information[::-1]\n",
        "            merged_secondary_information = merged_secondary_information[::-1]\n",
        "            wiki_match = wiki_match[::-1]\n",
        "\n",
        "        return secondary_information_string, secondary_information, merged_secondary_information, wiki_match\n",
        "    \n",
        "    def deep_check(self, information, dataset):\n",
        "        \"\"\"\n",
        "        Method is the main method, which uses sub-methods such as self.check_information and self.pairs_check, \n",
        "        to compare facts\n",
        "        :param information: data from the test dataset which is dictionary of all relevant data which were extracted \n",
        "                            from the related raw sentence\n",
        "        :param dataset: train dataset which is used as local database\n",
        "        :return: prediction which is either label (if match was found) or None (if match is not found or not reliable)\n",
        "        \"\"\"\n",
        "        category = information['category']\n",
        "        secondary_test = information['secondary_information_string']\n",
        "        sentence_test = information['raw_sentence']\n",
        "\n",
        "        for id, data in dataset.items():\n",
        "            secondary_truth = data['secondary_information_string']\n",
        "            sentence_truth = data['raw_sentence']\n",
        "            category_truth = data['category']\n",
        "            prediction = ''\n",
        "            if category in ['stars', 'spouse', 'better half']:\n",
        "                prediction = 'None'\n",
        "\n",
        "            else:\n",
        "                if category == category_truth:\n",
        "                    if self.check_information(data['label'], secondary_truth, secondary_test):\n",
        "                        self.counter += 1\n",
        "                        prediction = self.check_information(data['label'], secondary_truth, secondary_test)\n",
        "                        break\n",
        "                    else:\n",
        "                        prediction = 'None'\n",
        "                else:\n",
        "                    if self.pairs_check(category, category_truth, data['label'], secondary_truth, secondary_test):\n",
        "                        prediction = self.pairs_check(category, category_truth, data['label'], secondary_truth,\n",
        "                                                      secondary_test)\n",
        "                        self.counter += 1\n",
        "                        break\n",
        "                    else:\n",
        "                        prediction = 'None'\n",
        "\n",
        "        return prediction\n",
        "    \n",
        "    def check_information(self, label, secondary_truth, secondary_test):\n",
        "        \"\"\"\n",
        "        Method is used for local fact checking by comparing test data and train data (local database) before applying\n",
        "        wep-scrapping\n",
        "        :param label: label of fact (0.0 for False, 1.0 for True) from the train dataset (local database) \n",
        "        :param secondary_truth: list of main information of fact from the train dataset (local database)\n",
        "        :param secondary_test: list of main information of fact from the test dataset\n",
        "        :return: relevant label (True or False) or None if either facts do not match or match result is not reliable\n",
        "        \"\"\"\n",
        "        matches = [each_word for each_word in secondary_truth if each_word in secondary_test]\n",
        "        if len(matches) == len(secondary_truth):\n",
        "            return label\n",
        "        else:\n",
        "            if len(matches) == 1 and secondary_truth[0] in secondary_test:\n",
        "                if label == '1.0':\n",
        "                    return '0.0'\n",
        "                else:\n",
        "                    self.unbelievable += 1\n",
        "                    return None\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "    def pairs_check(self, category_test, category_truth, label, secondary_truth, secondary_test):\n",
        "        \"\"\"\n",
        "        Method is used to check whether there is category synonym matching in train and test facts. Pairs are checked\n",
        "        by using the list of pairs which was declared as self.pairs \n",
        "        :param category_test: category of fact from test dataset\n",
        "        :param category_truth: category of fact from train dataset (local database)\n",
        "        :param label: label of fact (0.0 for False, 1.0 for True) from the train dataset (local database) \n",
        "        :param secondary_truth: list of main information of fact from the train dataset (local database)\n",
        "        :param secondary_test: list of main information of fact from the test dataset\n",
        "        :return: prediction: result of pair checking, which is either label (True or False) or None\n",
        "        \"\"\"\n",
        "        relevant_pair = tuple()\n",
        "        for each_pair in self.pairs:\n",
        "            if category_test in each_pair:\n",
        "                relevant_pair = each_pair\n",
        "                break\n",
        "        prediction = None\n",
        "        if category_truth in relevant_pair:\n",
        "            if self.check_information(label, secondary_truth, secondary_test):\n",
        "                prediction = self.check_information(label, secondary_truth, secondary_test)\n",
        "        return prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***The Class of Scrapping is used to perform labelling such as:***\n",
        "# ***==> Web-scrapping for train dataset that not only predicts (labels) the train data, but also compute the accuracy of the fact-checking engine over train dataset;***\n",
        "# ***==> Web-scrapping and Local Database matching for test dataset predicts (labels) the test data.***"
      ],
      "metadata": {
        "id": "opq1WATuL2nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Scrapping:\n",
        "  def __init__(self, saved_path, synonyms, lcbase, typeset):\n",
        "    self.saved_path = saved_path\n",
        "    self.synonyms = synonyms\n",
        "    self.nlp = spacy.load('en_core_web_sm')\n",
        "    self.lcbase = lcbase\n",
        "    self.typeset = typeset\n",
        "\n",
        "  def get_pages(self, dataset, database=None):\n",
        "    \"\"\"\n",
        "    Method takes care of all the processes of the web scraping\n",
        "    :param dataset: dictionary of sentences (facts)\n",
        "    :return: dictionary of sentences with the info collected and labelled\n",
        "    \"\"\"\n",
        "    counter = 0\n",
        "    processing_bar = tqdm(enumerate(dataset.items()), 'Same as original: 0; M => True BUT O => False: 0; M => False BUT O => True: 0', len(dataset))\n",
        "    same_counter = 0\n",
        "    false_true = 0 # manually false but actually true\n",
        "    true_false = 0 # manually true but actually false\n",
        "    true_label = 0\n",
        "    false_label = 0\n",
        "    manual_label = str()\n",
        "    for idx, (each_id, each_information) in processing_bar:\n",
        "\n",
        "      results = each_information['results']\n",
        "      data = each_information['data']\n",
        "      category = data['category']\n",
        "      secondary_information = data['merged_secondary_information']\n",
        "      sec_inf_str = data['secondary_information_string']\n",
        "      wiki_list = data['wiki_match']\n",
        "\n",
        "      if self.valid(wiki_list):\n",
        "        data['sent_structure'] = 'valid'\n",
        "        search_info = self.generate_info(results, secondary_information, sec_inf_str)\n",
        "        html_info = self.set_link(search_info)\n",
        "        tables = self.extract_tables(html_info)\n",
        "        result_scrapping = self.scraping_check(tables, category)\n",
        "        \n",
        "        if self.typeset == 'train':\n",
        "          if self.is_fact(result_scrapping):\n",
        "            manual_label = '1.0'\n",
        "          else:\n",
        "            manual_label = '0.0'\n",
        "          manual_label, same, ft, tf = self.do_double(html_info, category, manual_label, data['label'])\n",
        "          data['predicted_label'] = manual_label\n",
        "\n",
        "          \n",
        "          if manual_label == '1.0':\n",
        "            true_label += 1\n",
        "          else:\n",
        "            false_label += 1\n",
        "          same_counter += same\n",
        "          true_false += tf\n",
        "          false_true += ft\n",
        "        else:\n",
        "          manual_label = self.lcbase.deep_check(data, database)\n",
        "          if manual_label != 'None':\n",
        "            if manual_label == '1.0':\n",
        "              true_label += 1\n",
        "            else:\n",
        "              false_label += 1\n",
        "          else:\n",
        "            if self.is_fact(result_scrapping) or self.get_text(html_info, category):\n",
        "              manual_label = '1.0'\n",
        "              true_label += 1\n",
        "            else:\n",
        "              manual_label = '0.0'\n",
        "              false_label += 1\n",
        "          data['predicted_label'] = manual_label\n",
        "          \n",
        "      else:\n",
        "        data['sent_structure'] = 'invalid'\n",
        "        manual_label = '0.0'\n",
        "        false_label += 1\n",
        "        if self.typeset == 'train':\n",
        "          if manual_label == data['label']:\n",
        "            same_counter += 1\n",
        "          else:\n",
        "            false_true += 1\n",
        "        data['predicted_label'] = manual_label\n",
        "      if self.typeset == 'train':\n",
        "        processing_bar.desc = f'General: True: {true_label}/{idx + 1}; ::::: False: {false_label}/{idx + 1} ::::: Same as original: {same_counter}/{idx+1}; ::::: Accuracy:{same_counter/(idx+1): .4f}; ::::: M => True BUT O => False: {false_true}/{idx+1}; ::::: M => False BUT O => True: {true_false}/{idx+1}'\n",
        "      else:\n",
        "        processing_bar.desc = f'Labelled test data: {idx + 1}/{len(dataset)}; ::::: True Labels: {true_label}/{(idx + 1)}; ::::: False Labels: {false_label}/{(idx + 1)}'\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "  def do_double(self, html_info, category, manual, original):\n",
        "    \"\"\"\n",
        "    Method applies text analysis in wiki as a double check of the fact\n",
        "    :param html_info: dictionary containing html page (soup format) and keyword --> {link 1: {'soup': soup, 'keyword': sec_info_2}, link2: {'soup': soup, 'keyword': sec_info_1}}\n",
        "    :param category: category\n",
        "    :param manual: label determined by using web scraping\n",
        "    :param original: label from train dataset\n",
        "    :return: strings & integers\n",
        "    \"\"\"\n",
        "    same_counter = 0\n",
        "    false_true = 0 # false positive\n",
        "    true_false = 0 # false negative\n",
        "    manual_label = manual\n",
        "    if manual == original:\n",
        "      same_counter += 1\n",
        "    else:\n",
        "      if self.get_text(html_info, category):\n",
        "        manual_label = '1.0'\n",
        "      else:\n",
        "        manual_label = '0.0'\n",
        "      \n",
        "      if manual_label == original:\n",
        "        same_counter += 1\n",
        "      else:\n",
        "        if original == 1:\n",
        "          false_true += 1\n",
        "        else:\n",
        "          true_false += 1\n",
        "    # print(f'same_counter:{same_counter}::::false_true: {false_true}:::::true_false: {true_false}')\n",
        "    return manual_label, same_counter, false_true, true_false\n",
        "    \n",
        "\n",
        "  def is_fact(self, scrape_result):\n",
        "    count = 0\n",
        "    for link, information in scrape_result.items():\n",
        "      if information['match']:\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "\n",
        "  def get_text(self, information, category):\n",
        "    \"\"\"\n",
        "    Extracts text from wiki page and parse sentences to isolate information\n",
        "    :param information: dictionary containing html page (soup format) and keyword\n",
        "    :param category: category\n",
        "    :return: boolean (True if text contains information, False if not)\n",
        "    \"\"\"\n",
        "\n",
        "    main_list = list()\n",
        "    for each_link, each_information in information.items():\n",
        "      text = str()\n",
        "      soup = each_information['soup']\n",
        "      double_check = list()\n",
        "      if soup != 'invalid_link':\n",
        "        for each_paragraph in soup.find_all('p'):\n",
        "          text += each_paragraph.text\n",
        "        raw_text = re.sub(r\"\\[.*?\\]+\", '', text)\n",
        "        doc = self.nlp(raw_text)\n",
        "        sentences = [each_sentence.text for each_sentence in doc.sents]\n",
        "        for each_sentence in sentences:\n",
        "          if re.search(each_information['keyword_str'], each_sentence, re.IGNORECASE): \n",
        "            for each_synonym in self.synonyms[category]:\n",
        "              if re.search(each_synonym, each_sentence, re.IGNORECASE):\n",
        "                double_check.append(each_sentence)\n",
        "\n",
        "      main_list.extend(double_check)\n",
        "\n",
        "\n",
        "    if main_list:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "\n",
        "\n",
        "  def extract_match(self, match_list, keyword):\n",
        "    \"\"\"\n",
        "    Retrieve info of a cell when the label is a synonym of the category word    \n",
        "\n",
        "    :param match_list: list of all cells of the infobox table\n",
        "    :return: list of all matches\n",
        "    \"\"\"\n",
        "    info_list = list()\n",
        "    for label, info in match_list:\n",
        "      for each_part in keyword:\n",
        "        if re.search(each_part, info, re.IGNORECASE):\n",
        "          info_list.append(info)\n",
        "        \n",
        "    return info_list\n",
        "  \n",
        "  def scraping_check(self, information, category):\n",
        "    \"\"\"\n",
        "    Checks if there are match(es) in table cells\n",
        "    :param information: infobox tables\n",
        "    :param category: category\n",
        "    :return: tables\n",
        "    \"\"\"\n",
        "    match_dict = dict()\n",
        "    count = 0\n",
        "    for each_link, each_information in information.items():\n",
        "      \n",
        "      box_labels = list()               \n",
        "\n",
        "\n",
        "      keyword = each_information['keyword'].split(', ') if re.search(',', each_information['keyword'], re.IGNORECASE) else [each_information['keyword']]\n",
        "      if each_information['table'] != 'no_table':\n",
        "        for box_label, box_info in each_information['table'].items():\n",
        "          category_match = [(box_label, box_info) for each_synonym in self.synonyms[category] if re.search(each_synonym, box_label, re.IGNORECASE) != None]\n",
        "          if category_match:\n",
        "            each_information['match'] = self.extract_match(category_match, keyword)\n",
        "            if not each_information['match']:\n",
        "              each_information['match'] = None\n",
        "            else:\n",
        "              break\n",
        "          else:\n",
        "            each_information['match'] = None\n",
        "      else:\n",
        "        each_information['match'] = None\n",
        "\n",
        "    return information\n",
        "     \n",
        "\n",
        "  def extract_tables(self, html_info):\n",
        "    \"\"\"\n",
        "    Extracts data from the infobox table if it exists in Wikipedia page\n",
        "    :param html_info: dictionary containing html page (soup format) and keyword --> {link 1: {'soup': soup, 'keyword': sec_info_2}, link2: {'soup': soup, 'keyword': sec_info_1}}\n",
        "    :return: dictionary containing the infobox table and the keyword --> {'table': table_info, 'keyword': each_information['keyword']}\n",
        "    \"\"\" \n",
        "    data_dict = dict()\n",
        "    for each_link, each_information in html_info.items():\n",
        "      table_info = list()\n",
        "      soup = each_information['soup']\n",
        "      if soup == 'invalid_link':\n",
        "        table_info = 'no_table'\n",
        "      else:\n",
        "        tables = soup.findChildren('table')\n",
        "        table_infobox = None\n",
        "        if not tables:\n",
        "          \n",
        "          table_info = 'no_table'\n",
        "          \n",
        "        else:\n",
        "          for table in tables:\n",
        "            if table.has_attr('class') and table['class'][0] == 'infobox':\n",
        "\n",
        "              table_infobox = table\n",
        "                           \n",
        "              break\n",
        "          if table_infobox:\n",
        "            table_dict = dict()\n",
        "            counter = 0\n",
        "            for each_row in table_infobox.findChildren('tr'):\n",
        "\n",
        "              if len(each_row)>1:\n",
        "                counter += 1\n",
        "                cols = [each.text for each in each_row.find_all(['th', 'td'])]\n",
        "                for idx in range(1, len(cols)):\n",
        "                  table_dict[cols[0].replace('\\xa0', ' ')] = unicodedata.normalize(\"NFKC\", cols[idx].replace('\\u200b', ''))\n",
        "            table_info = table_dict if counter else 'no_table'\n",
        "          else:\n",
        "            table_info = 'no_table'\n",
        "        data_dict[each_link] = {'table': table_info, 'keyword': each_information['keyword']}\n",
        "    return data_dict\n",
        "\n",
        "  def save_and_load(self, dataset, database=None):\n",
        "    \"\"\"\n",
        "    Load file (if it exists) and save the content in variable otherwise it will create the file with the data and save data in varaible aswell\n",
        "    :return: dictionary containing the data\n",
        "    \"\"\" \n",
        "    info_dict = dict()\n",
        "    filename = os.path.join(self.saved_path, f'labelled_{self.typeset}.json')\n",
        "    if os.path.exists(filename):\n",
        "      with open(filename) as saved_file:\n",
        "        info_dict = json.load(saved_file)\n",
        "    else:\n",
        "      info_dict = self.get_pages(dataset, database)\n",
        "      with open(filename, 'w') as saved_file:\n",
        "        json.dump(info_dict, saved_file)\n",
        "    return info_dict\n",
        "\n",
        "  def valid(self, info):\n",
        "    \"\"\"\n",
        "    Checks the validity of the list of wikipedia search result list\n",
        "    :param info: list of search results (strings)\n",
        "    :return: boolean (True if text contains usable information, False if not)\n",
        "    \"\"\"\n",
        "    req = [each for each in info if each == 'sorry not found']\n",
        "    if len(req) == len(info):\n",
        "      return False\n",
        "    else:\n",
        "      return True\n",
        "\n",
        "  def generate_info(self, result, sec_info, sec_str_info):\n",
        "    \"\"\"\n",
        "    Method extracts dictionary for looking for information as below:    \n",
        "\n",
        "    :param result: dict : {merged_secondary_info: relevant and correct link} Note that each dict has to 2 keys corresponding to each information that sentence provides\n",
        "    :param sec_info: list: [sec_info_1, sec_info_2] list of secondary information that was provided by sentences\n",
        "    :return: research_dict: dict {link 1: sec_info_2, link2: sec_info_1} includes link and corresponding information will be looked for\n",
        "    \"\"\"\n",
        "    research_dict = dict()\n",
        "    for each in result.keys():\n",
        "      for each_sec, each_str in zip(sec_info, sec_str_info):\n",
        "        if each != each_sec:\n",
        "          research_dict[result[each]] = (each_sec, each_str)\n",
        "    return research_dict\n",
        "  \n",
        "  def set_link(self, info):\n",
        "    \"\"\"\n",
        "    Extract relevant BeautifulSoup object for requested page    \n",
        "\n",
        "    :param info: dict {link 1: sec_info_2, link2: sec_info_1} includes link and corresponding information will be looked for\n",
        "    :return: info: dict {link 1: {'soup': soup, 'keyword': sec_info_2}, link2: {'soup': soup, 'keyword': sec_info_1}} includes link and corresponding information will be looked for\n",
        "    \"\"\"\n",
        "    soup = ''\n",
        "    for each_key in info.keys():\n",
        "      if each_key == 'sorry_not_found':\n",
        "        soup = 'invalid_link'\n",
        "        keyword = info[each_key]\n",
        "        \n",
        "      else:\n",
        "        setURL = urllib.parse.quote(each_key)\n",
        "      # Send request and retrieve response\n",
        "        myURL = \"https://en.wikipedia.org/wiki/\" + setURL\n",
        "        req = urllib.request.Request(myURL)\n",
        "        with urllib.request.urlopen(req) as response:\n",
        "          the_page = response.read()\n",
        "        soup = BeautifulSoup(the_page)\n",
        "        keyword = info[each_key]\n",
        "\n",
        "      info[each_key] = {'soup': soup, 'keyword': keyword[0], 'keyword_str': keyword[1]}\n",
        "    return info\n",
        "\n"
      ],
      "metadata": {
        "id": "DzUV0XsyaxUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3eElKD8WoCr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "5e30449e04c7447397d45dbf6ccb61da",
            "186acaa3cb304d30a59bdcce3f2fef5f",
            "e2a478d7c6394ceaa5cfdcec406609b2",
            "9b1ea9d4cab44bb9ac9a317101b1d55d",
            "4bf9feea06cf4c85ad337a96d1c4d229",
            "b4b9b0fd23fb4c61a003021a305cda48",
            "83374e02cbb145568499f71b6a6780ec",
            "fa05c87b0b244eef9b0dbcc3b1bbc4bf",
            "8271805643ea44a2ae879865490c7999",
            "b54bc52093d34d3bbe0f77c97d182820",
            "8b7bc7204357405884b5228285e92fdb"
          ]
        },
        "outputId": "02c55dfb-11d7-4181-9003-f6cdbe3eb201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e30449e04c7447397d45dbf6ccb61da",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Data is collected and processed:   0%|          | 0/1234 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# mount drive and build necessary folders and files\n",
        "drive.mount('/content/drive')\n",
        "folder_name = 'SNLP_Fact_Checking-main/SNLP'\n",
        "directory_folder = f'drive/MyDrive/{folder_name}' \n",
        "saved_path = os.path.join(directory_folder, 'saved_path')\n",
        "train_file = os.path.join(directory_folder, 'SNLP2020_training.tsv')\n",
        "test_file = os.path.join(directory_folder, 'SNLP2020_test.tsv')\n",
        "\n",
        "# generate folder to save and load required files\n",
        "if not os.path.exists(saved_path):\n",
        "  os.makedirs(saved_path)\n",
        "\n",
        "\n",
        "# declaration of raw dataset collection\n",
        "data_collector_train = GatherData(train_file, saved_path, 'train')\n",
        "categories = data_collector_train.extract_categories()\n",
        "data_collector_test = GatherData(test_file, saved_path, 'test', categories=categories)\n",
        "\n",
        "# raw dataset before processing\n",
        "train_raw_dataset = data_collector_train.save_and_load()\n",
        "test_raw_dataset = data_collector_test.save_and_load()\n",
        "\n",
        "# process raw data and gather relevant information\n",
        "main_categories = categories['bigram_categories'] + categories['unigram_categories']\n",
        "processor_train = ProcessData(saved_path, 'train_process', data_collector_train)\n",
        "processor_test = ProcessData(saved_path, 'test_process', data_collector_test)\n",
        "\n",
        "# processed datasets and extra categories\n",
        "train_dataset, extra_categories = processor_train.save_and_load(main_categories)\n",
        "test_dataset, extra_categories = processor_test.save_and_load(main_categories, extra_categories)\n",
        "\n",
        "# visualization of the dataset\n",
        "all_categories = main_categories + extra_categories\n",
        "visualizer = VisualizeData(train_dataset)\n",
        "ne, textinfo = visualizer.elements_by_categories(all_categories)\n",
        "visualizer.plot_bars(ne, textinfo, 'Train')\n",
        "\n",
        "visualizer_test = VisualizeData(test_dataset)\n",
        "ne_test, textinfo_test = visualizer_test.elements_by_categories(all_categories)\n",
        "visualizer.plot_bars(ne_test, textinfo_test, 'Test')\n",
        "\n",
        "# generating links for each information in each data that train and test dataset includes \n",
        "resulting_train_file = 'results_train.json'\n",
        "resulting_test_file = 'results_test.json'\n",
        "scrapper_train = GetLinks(saved_path, train_dataset, data_collector_train.stop_words)\n",
        "result_train = scrapper_train.save_and_load(resulting_train_file)\n",
        "scrapper_test = GetLinks(saved_path, test_dataset, data_collector_test.stop_words)\n",
        "result_test = scrapper_test.save_and_load(resulting_test_file)\n",
        "\n",
        "# dictionary of synonyms of the corresponding categories\n",
        "synonymDB = {\n",
        "              'nascence place': ['nascence place', 'birth', 'born'],\n",
        "              'death place': ['death place', 'dead', 'die', 'last place'],\n",
        "              'stars': ['star', 'starring', 'actor'],\n",
        "              'team': [\"\\d{4}\"],\n",
        "              'squad': [\"\\d{4}\"],\n",
        "              'author': [\"author\", 'writer', \"novelist\", 'playwright', 'creator'],\n",
        "              'foundation place': ['founded', 'foundation', 'base', 'headquarter', 'innnovation place'],\n",
        "              'award': ['award', 'prize', 'honour', 'decoration', 'reward', 'medal'],\n",
        "              'last place': ['last place', 'death place', 'dead', 'die'],\n",
        "              'innovation place': ['innovation place', 'innovation', 'revolution', 'transformation', 'revolution', 'headquarter', 'founded', 'foundation place'],\n",
        "              'better half': ['better half', 'partner', 'wife', 'husband', 'spouse'], \n",
        "              'honour': ['honour', 'distinction', 'tribute', 'award', 'prize', 'decoration', 'reward'], \n",
        "              'subsidiary': ['subsidiar', 'successor', 'parent', 'predecessor', 'developer', 'acquire'], \n",
        "              'generator': ['generator', 'author'],\n",
        "              'subordinate': ['subordinate', 'fate', 'acquire', 'acquisition'],\n",
        "              'spouse': ['spouse', 'partner', 'wife', 'husband', 'better half'],\n",
        "              'birth place': ['birth place', 'nascence place', 'birth', 'born']  \n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_all = result_train['all']\n",
        "test_all = result_test['all']\n",
        "\n",
        "database = LocalDatabase(saved_path)\n",
        "train_database = database.generate_database(train_all)\n",
        "print(len(train_raw_dataset))"
      ],
      "metadata": {
        "id": "UPmKTJm-46e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b9ccd82-0bff-47f3-8506-0f7d1a4d77b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Declaration of web scrapper for test and train datasets, and prediction of train and test labels\n",
        "testscrape = Scrapping(saved_path, synonymDB, database, 'test')\n",
        "trainscrape = Scrapping(saved_path, synonymDB, None, 'train')\n",
        "\n",
        "labelled_train = trainscrape.save_and_load(train_all)\n",
        "labelled_test = testscrape.save_and_load(test_all, train_database)\n"
      ],
      "metadata": {
        "id": "luFLVz5OoNPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this code cell was implemented as an extra work to compute accuracy of some test data which have already been labelled by using local database (reliable results)\n",
        "proces_check = tqdm(iterable=enumerate(test_all.items()), desc=f\"Checked and labelled: 0 Checked but not labelled: 0\", total=len(test_all))\n",
        "samples = dict()\n",
        "for idx, (id, value) in proces_check:\n",
        "  info = value['data']\n",
        "  info['label'] = database.deep_check(info, train_database)\n",
        "  \n",
        "  if info['label'] != 'None':\n",
        "    samples[id] = value\n",
        "  else:\n",
        "    continue\n",
        "  proces_check.desc = f\"Checked and labelled: {database.counter} Checked but not labelled: {database.unbelievable}\"\n",
        "samples = trainscrape.get_pages(samples)\n"
      ],
      "metadata": {
        "id": "U0fXkUTdIWyy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165,
          "referenced_widgets": [
            "18d3e386370d4f56b49bc1449000fda7",
            "e3b8ecaa17e4456b8d6abd3680a3508d",
            "f07e687a73154f78b77bdbdf5155ac54",
            "129d56e475c745a89a1b6dbb11a91ac6",
            "35630e02b73e428887fa1063e6ba0d76",
            "1551ee0398054464972ea1268972c6fb",
            "c01fcaa967b240bf93e8fc639a910493",
            "a0b4b2252d324cd693c0d72da235bd6d",
            "c1603a3daaff4beb93477788cf016e36",
            "59b84f46022045aa8dde7329b03b3f9a",
            "3f7f0eb0fe41494086a8b5de410d26e3",
            "0691f4d2899a46ffb3d216c097c6af96",
            "8c6dde3e0e2349438aa758b3c6e357a0",
            "fc3b02cf20c94d1695029bf4fd441062",
            "f36ef334f3124cb59c9190f2bd0b97fb",
            "1861d1bb1a6141dc9220cacf49f5c4b2",
            "692c26994c254bfab893f816cd2850a2",
            "c0d5f68a794544dcbb5f63ee36ab0b69",
            "ce5434351e5e48ef9088b232068af9c9",
            "57b52b956b914af0a4120fbced4ce064",
            "8cab0340b7f640cf8cef5e89dee881f9",
            "6875cfedfb6f4682a14e161b04e1a738"
          ]
        },
        "outputId": "d1268d6f-a57d-4c75-8e4f-9fe71ae74cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18d3e386370d4f56b49bc1449000fda7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Checked and labelled: 0 Checked but not labelled: 0:   0%|          | 0/1342 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0691f4d2899a46ffb3d216c097c6af96",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Same as original: 0; M => True BUT O => False: 0; M => False BUT O => True: 0:   0%|          | 0/145 [00:00<?"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(dataset, filetype):\n",
        "  name_file = 'result.ttl' if filetype == 'test' else 'result_train.ttl'\n",
        "  filename = os.path.join(saved_path, name_file)\n",
        "  if not os.path.exists(filename):\n",
        "    with open(filename, 'a') as result_file:\n",
        "      for each_id, each_dict in dataset.items():\n",
        "        data = each_dict['data']\n",
        "      # print(f\"{each_id} : {data['predicted_label']}\")\n",
        "        result_file.write(f\"<http://swc2017.aksw.org/task2/dataset/{each_id}> <http://swc2017.aksw.org/hasTruthValue> \\\"{float(data['predicted_label'])}\\\"^^<http://www.w3.org/2001/XMLSchema#double> .\\n\")\n",
        "    print(f'{name_file} was already generated and saved into {saved_path} folder')\n",
        "  else:\n",
        "    print(f'{name_file} is already generated and saved in {saved_path} folder')\n",
        "\n",
        "generate_output(labelled_test, 'test')\n",
        "generate_output(labelled_train, 'train')\n"
      ],
      "metadata": {
        "id": "QKIuGQblThKp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342e3752-9775-4c80-e41d-c8a91cf327e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1169\n",
            "1234\n",
            "0.9473257698541329\n",
            "result.ttl was already generated and saved into drive/MyDrive/SNLP/saved_path folder\n",
            "result_train.ttl was already generated and saved into drive/MyDrive/SNLP/saved_path folder\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "fact_engine.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "18d3e386370d4f56b49bc1449000fda7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e3b8ecaa17e4456b8d6abd3680a3508d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f07e687a73154f78b77bdbdf5155ac54",
              "IPY_MODEL_129d56e475c745a89a1b6dbb11a91ac6",
              "IPY_MODEL_35630e02b73e428887fa1063e6ba0d76"
            ]
          }
        },
        "e3b8ecaa17e4456b8d6abd3680a3508d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f07e687a73154f78b77bdbdf5155ac54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1551ee0398054464972ea1268972c6fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Checked and labelled: 145 Checked but not labelled: 151: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c01fcaa967b240bf93e8fc639a910493"
          }
        },
        "129d56e475c745a89a1b6dbb11a91ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0b4b2252d324cd693c0d72da235bd6d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1342,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1342,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c1603a3daaff4beb93477788cf016e36"
          }
        },
        "35630e02b73e428887fa1063e6ba0d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_59b84f46022045aa8dde7329b03b3f9a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1342/1342 [00:01&lt;00:00, 678.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f7f0eb0fe41494086a8b5de410d26e3"
          }
        },
        "1551ee0398054464972ea1268972c6fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c01fcaa967b240bf93e8fc639a910493": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a0b4b2252d324cd693c0d72da235bd6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c1603a3daaff4beb93477788cf016e36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "59b84f46022045aa8dde7329b03b3f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f7f0eb0fe41494086a8b5de410d26e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0691f4d2899a46ffb3d216c097c6af96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8c6dde3e0e2349438aa758b3c6e357a0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_fc3b02cf20c94d1695029bf4fd441062",
              "IPY_MODEL_f36ef334f3124cb59c9190f2bd0b97fb",
              "IPY_MODEL_1861d1bb1a6141dc9220cacf49f5c4b2"
            ]
          }
        },
        "8c6dde3e0e2349438aa758b3c6e357a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc3b02cf20c94d1695029bf4fd441062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_692c26994c254bfab893f816cd2850a2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "General: True: 8/145; ::::: False: 137/145 ::::: Same as original: 137/145; ::::: Accuracy: 0.9448; ::::: M =&gt; True BUT O =&gt; False: 0/145; ::::: M =&gt; False BUT O =&gt; True: 8/145: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0d5f68a794544dcbb5f63ee36ab0b69"
          }
        },
        "f36ef334f3124cb59c9190f2bd0b97fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ce5434351e5e48ef9088b232068af9c9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 145,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 145,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_57b52b956b914af0a4120fbced4ce064"
          }
        },
        "1861d1bb1a6141dc9220cacf49f5c4b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8cab0340b7f640cf8cef5e89dee881f9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 145/145 [02:17&lt;00:00,  1.01it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6875cfedfb6f4682a14e161b04e1a738"
          }
        },
        "692c26994c254bfab893f816cd2850a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0d5f68a794544dcbb5f63ee36ab0b69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce5434351e5e48ef9088b232068af9c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "57b52b956b914af0a4120fbced4ce064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8cab0340b7f640cf8cef5e89dee881f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6875cfedfb6f4682a14e161b04e1a738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5e30449e04c7447397d45dbf6ccb61da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_186acaa3cb304d30a59bdcce3f2fef5f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e2a478d7c6394ceaa5cfdcec406609b2",
              "IPY_MODEL_9b1ea9d4cab44bb9ac9a317101b1d55d",
              "IPY_MODEL_4bf9feea06cf4c85ad337a96d1c4d229"
            ]
          }
        },
        "186acaa3cb304d30a59bdcce3f2fef5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e2a478d7c6394ceaa5cfdcec406609b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b4b9b0fd23fb4c61a003021a305cda48",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Data is collected and processed: 3851499:  34%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_83374e02cbb145568499f71b6a6780ec"
          }
        },
        "9b1ea9d4cab44bb9ac9a317101b1d55d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fa05c87b0b244eef9b0dbcc3b1bbc4bf",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1234,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 421,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8271805643ea44a2ae879865490c7999"
          }
        },
        "4bf9feea06cf4c85ad337a96d1c4d229": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b54bc52093d34d3bbe0f77c97d182820",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 421/1234 [00:22&lt;00:41, 19.38it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8b7bc7204357405884b5228285e92fdb"
          }
        },
        "b4b9b0fd23fb4c61a003021a305cda48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "83374e02cbb145568499f71b6a6780ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fa05c87b0b244eef9b0dbcc3b1bbc4bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8271805643ea44a2ae879865490c7999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b54bc52093d34d3bbe0f77c97d182820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8b7bc7204357405884b5228285e92fdb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}